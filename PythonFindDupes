import os
import hashlib
import zipfile
from io import BytesIO

def hash_stream(stream, chunk_size=8192):
    h = hashlib.sha256()
    while chunk := stream.read(chunk_size):
        h.update(chunk)
    return h.hexdigest()

def process_zip(zip_bytes, parent_label, hashes):
    with zipfile.ZipFile(BytesIO(zip_bytes)) as z:
        for member in z.namelist():
            if member.endswith("/"):
                continue

            data = z.read(member)
            label = f"{parent_label} :: {member}"

            # If the member is itself a ZIP, recurse
            if zipfile.is_zipfile(BytesIO(data)):
                process_zip(data, label, hashes)
            else:
                h = hashlib.sha256(data).hexdigest()
                hashes.setdefault(h, []).append(label)

def scan_path(path, hashes):
    if zipfile.is_zipfile(path):
        with open(path, "rb") as f:
            data = f.read()
        process_zip(data, path, hashes)
        return

    # Normal file
    with open(path, "rb") as f:
        h = hash_stream(f)
    hashes.setdefault(h, []).append(path)

def scan_folder(root):
    hashes = {}
    for dirpath, _, filenames in os.walk(root):
        for name in filenames:
            full = os.path.join(dirpath, name)
            scan_path(full, hashes)
    return hashes

def report_duplicates(hashes):
    for h, files in hashes.items():
        if len(files) > 1:
            print("\nDuplicate group:")
            for f in files:
                print("  ", f)

if __name__ == "__main__":
    root_folder = r"/path/to/scan"
    hashes = scan_folder(root_folder)
    report_duplicates(hashes)
